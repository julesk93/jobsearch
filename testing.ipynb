{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoodJobs.eu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL of the website you want to extract text from\n",
    "url = 'https://goodjobs.eu/jobs/bereichsleiterin-wirtschaft-und-finanzen-kleeblatt-pflegeheime-ggmbh'\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the HTML content of the webpage\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_filename():\n",
    "    # Extract titles\n",
    "    title = soup.title.text if soup.title else \"No title found\"\n",
    "    title.strip()\n",
    "    cleaned_title = re.sub(r'[^\\w\\s]', '', title.strip())\n",
    "    cleaned_title = cleaned_title.replace('GoodJobs', '').strip()\n",
    "    return cleaned_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get lists nested under different sections, e.g. Profile, Tasks, etc.\n",
    "def get_lists(section):\n",
    "    list = soup.find('section', id=section).find('div', class_='text-style-body text-responsive-xs checkmark-list').find_all('li')\n",
    "    return list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_ansprechpartnerin():\n",
    "#     keywords = [\"ANSPRECHPARTNERIN\", \"ANSPRECHPARTNER\", \"Ansprechpartner\"]\n",
    "#     # combine for or condition\n",
    "#     pattern = re.compile(\"|\".join(keywords))\n",
    "\n",
    "#     # Find all <strong> elements containing any of the keywords\n",
    "#     matching_elements = soup.find_all('strong', string=pattern)\n",
    "\n",
    "#     # Get the text from the <p> element following each matching <strong>\n",
    "#     for strong_element in matching_elements:\n",
    "#         parent_p = strong_element.find_parent('p')\n",
    "#         if parent_p:\n",
    "#             next_p = parent_p.find_next_sibling('p')\n",
    "#             if next_p:\n",
    "#                 next_next_p = next_p.find_next_sibling('p')\n",
    "#                 if next_next_p:\n",
    "#                     return next_p.text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ansprechpartnerin():\n",
    "    # Find the <div> element with class \"col-span-full lg:col-span-8 lg:row-start-3 border-r-2\"\n",
    "    div_element = soup.find('div', class_='col-span-full lg:col-span-8 lg:row-start-3 border-r-2')\n",
    "\n",
    "    # If the <div> element is found, find the <h2> element within it\n",
    "    if div_element:\n",
    "        h2_element = div_element.find('h2', class_='text-style-headline text-responsive-l')\n",
    "        # If the <h2> element is found, print its text\n",
    "        if h2_element:\n",
    "            return h2_element.get_text().replace('\\n', '').replace('            ',' ').strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stefan Ebert'"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_ansprechpartnerin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_description(section_title):\n",
    "    intro_section = soup.find('section', id=section_title)\n",
    "    # If the section is found, extract all text within it\n",
    "    if intro_section:\n",
    "        nested_div = intro_section.find('div', class_='text-style-body text-responsive-xs checkmark-list')\n",
    "        if nested_div:\n",
    "            paragraphs = nested_div.find_all('p')\n",
    "            # Extract text from each <p> element\n",
    "            paragraph_texts = [p.get_text(strip=True) for p in paragraphs]\n",
    "            # Remove empty strings\n",
    "            cleaned_data = [item for item in paragraph_texts if item != '']\n",
    "            return cleaned_data\n",
    "        else:\n",
    "            return []\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract job criteria e.g. Starting Date, Location, etc.\n",
    "def extract_job_criteria(keyword):\n",
    "    # Find the <strong> tag containing the keyword\n",
    "    strong_tag = soup.find('strong', string=keyword)\n",
    "    # Extract the text from the next sibling and remove \": \" prefix\n",
    "    if strong_tag:\n",
    "        next_sibling = strong_tag.find_next_sibling(string=True)\n",
    "        if next_sibling:\n",
    "            text = next_sibling.strip().split(': ')[1]  # Remove \": \" prefix\n",
    "            return text\n",
    "    # Return None if keyword not found or text extraction fails\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_company():\n",
    "    # Find the section with the id \"company\"\n",
    "    company_section = soup.find('section', id='company')\n",
    "    # Find the first h2 within the company section\n",
    "    if company_section:\n",
    "        first_h2 = company_section.find('h2')\n",
    "        if first_h2:\n",
    "            return first_h2.text\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_split(search_term):\n",
    "    # Find the <p> tag containing the search string\n",
    "    p_tag = soup.find('p', string=lambda text: text and search_term in text)\n",
    "\n",
    "    # If the <p> tag is found, return its text content\n",
    "    if p_tag:\n",
    "        p_text = p_tag.get_text()\n",
    "        parts = p_text.split(search_term)\n",
    "        # Extract the date part\n",
    "        date = parts[-1].strip()\n",
    "        return date\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_date(date_string):\n",
    "    # Split the date string based on the \".\" separator\n",
    "    parts = date_string.split('.')\n",
    "    # Rearrange the parts and concatenate them with the desired format\n",
    "    transformed_date = \"[[\" + '-'.join(parts[::-1]) + \"]]\"\n",
    "    return transformed_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = f\"{clean_filename()}.md\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobtyp = extract_job_criteria(\"Job-Typ\")\n",
    "starting_date = extract_split(\"Arbeitsbeginn: \")\n",
    "company = extract_company()\n",
    "deadline = transform_date(extract_split(\"Job online bis\"))\n",
    "ansprechpartnerin = extract_ansprechpartnerin()\n",
    "berufserfahrung = extract_split(\"Berufserfahrung: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = f\"\"\"\\\n",
    "type:: stellenausschreibung\n",
    "institution:: {company}\n",
    "Teilzeit:: {jobtyp}\n",
    "status:: offen\n",
    "starting_date:: {starting_date}\n",
    "kommentar:: offen\n",
    "deadline:: {deadline}\n",
    "ansprechpartner:: {ansprechpartnerin}\n",
    "website:: [Stellenausschreibung]({url})\n",
    "berufserfahrung:: {berufserfahrung}\n",
    "kennziffer:: offen\n",
    "email:: offen\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Werden Sie Teil der Kleeblatt Familie. Wir freuen uns auf Sie!',\n",
       " 'Ihre Daten werden nach Abschluss des Verfahrens binnen einer Frist von drei Monaten vernichtet.']"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_description(\"bewerbungsprozess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_description_list = extract_description(\"intro\")\n",
    "profile_list = get_lists(\"anforderungen\")\n",
    "task_list = get_lists(\"aufgaben\")\n",
    "benefits_list = get_lists(\"benefits\")\n",
    "bewerbungsprozess_list = extract_description(\"bewerbungsprozess\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_filename, 'w', encoding='utf-8') as markdown_file:\n",
    "    # Write tags to markdown\n",
    "    markdown_file.write(tags)\n",
    "    # Write Profil / Requirements to markdown\n",
    "    markdown_file.write(f\"- ## Jobbeschreibung\\n\")\n",
    "    for item in job_description_list:\n",
    "        markdown_file.write(f\"\\t- {item}\\n\")\n",
    "    markdown_file.write(\"\\n\")\n",
    "    markdown_file.write(f\"- ## Profil\\n\")\n",
    "    for item in profile_list:\n",
    "        markdown_file.write(f\"\\t- {item.text.strip()}\\n\")\n",
    "    # Write Tasks / Aufgaben to markdown\n",
    "    markdown_file.write(f\"- ## Aufgaben\\n\")\n",
    "    for item in task_list:\n",
    "        markdown_file.write(f\"\\t- {item.text.strip()}\\n\")\n",
    "    # Write Benefits to markdown\n",
    "    markdown_file.write(f\"- ## Benefits\\n\")\n",
    "    for item in task_list:\n",
    "        markdown_file.write(f\"\\t- {item.text.strip()}\\n\")\n",
    "    markdown_file.write(f\"- ## Bewerbungsprozess\\n\")\n",
    "    for item in bewerbungsprozess_list:\n",
    "        markdown_file.write(f\"\\t- {item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import different job offers for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-18 15:40:43.845 Python[3054:55539] +[CATransaction synchronize] called within transaction\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No file selected.\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Create a Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# Open a file dialog for the user to select the HTML file\n",
    "html_file_path = filedialog.askopenfilename(title=\"Select HTML file\", filetypes=((\"HTML files\", \"*.html\"), (\"All files\", \"*.*\")))\n",
    "\n",
    "# Check if a file was selected\n",
    "if html_file_path:\n",
    "    # Read the HTML content from the selected file\n",
    "    with open(html_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        html_content = file.read()\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Now you can use 'soup' to process the HTML content further\n",
    "    print(\"HTML file parsed successfully.\")\n",
    "else:\n",
    "    print(\"No file selected.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the HTML file and read its content\n",
    "with open(\"/Users/juliankilchling/Downloads/Data Analyst (f_m_d) Scholz & Friends LinkedIn.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust\n",
    "with open(\"/Users/juliankilchling/Downloads/(1) LinkedIn.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ey\n",
    "html = \"/Users/juliankilchling/Downloads/EY senior consultant.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funke\n",
    "html = \"/Users/juliankilchling/Downloads/(Senior Data Analyst (m_w_d) FUNKE LinkedIn.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust\n",
    "html = \"/Users/juliankilchling/Downloads/(1) LinkedIn.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scholz & Friends\n",
    "html = \"/Users/juliankilchling/Downloads/Data Analyst (f_m_d) Scholz & Friends.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(html, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dictionary to store results of webscrape\n",
    "linkedin_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Data Analyst (f/m/d)'}\n"
     ]
    }
   ],
   "source": [
    "# Get Job Title => WORKS\n",
    "linkedin_dict[\"title\"] = soup.find('h1', class_='t-24 t-bold job-details-jobs-unified-top-card__job-title').text.strip()\n",
    "print(linkedin_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get company name\n",
    "job_details_div = soup.find('div', {'class':'job-details-jobs-unified-top-card__primary-description-without-tagline mb2'})\n",
    "linkedin_dict['company'] = job_details_div.find('a').text.strip()\n",
    "\n",
    "# Get publication date\n",
    "linkedin_dict['pub_date'] = job_details_div.find(lambda tag: tag.name == 'span' and 'Vor' in tag.get_text()).text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_details_2 = soup.find('li', {'class':'job-details-jobs-unified-top-card__job-insight job-details-jobs-unified-top-card__job-insight--highlight'})\n",
    "\n",
    "#find out the job type (full-time, part-time, etc.)\n",
    "try:\n",
    "    linkedin_dict[\"jobtyp\"] = job_details_2.find_all('span', {'class':'job-details-jobs-unified-top-card__job-insight-view-model-secondary'})[0].find('span', {'aria-hidden':'true'}).text.strip()\n",
    "except AttributeError:\n",
    "    linkedin_dict[\"jobtyp\"] = \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Data Analyst (f/m/d)',\n",
       " 'company': 'Scholz & Friends',\n",
       " 'pub_date': 'Vor 1 Woche',\n",
       " 'jobtyp': 'Vollzeit'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Testing\n",
    "linkedin_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Data Analyst (f/m/d)', 'company': 'Scholz & Friends', 'pub_date': 'Vor 1 Woche', 'jobtyp': 'Vollzeit', 'Bestehende Kenntnisse': 'Datenanalytik, Datenvisualisierung, Looker (Software), Soziale Medien', 'Fehlende Kenntnisse': 'Benchmarking, Business Insights, Dashboard, Handlungsentwicklung, Kennzahlen, Round Tables'}\n"
     ]
    }
   ],
   "source": [
    "# Get skills from LinkedIn Job Posting\n",
    "qualifications_div = soup.find('div', id='how-you-match-card-container')\n",
    "qualifications = qualifications_div.find_all('h3', {'class':'t-14 t-bold'})\n",
    "\n",
    "for qual in qualifications:\n",
    "    key_qual = qual.find_next_sibling(\"a\").text.replace(\" und\",\",\").replace(\"\\n\",\"\").strip()\n",
    "    qual_stripped = qual.text.replace(\"Kenntnisse fehlen auf Ihrem Profil\",\"Fehlende Kenntnisse\").replace(\"Kenntnisse auf Ihrem Profil\",\"Bestehende Kenntnisse\").strip()[2:].lstrip()\n",
    "    linkedin_dict[qual_stripped] = key_qual\n",
    "print(linkedin_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get job description section\n",
    "# Find the article element with the specified class\n",
    "job_description = soup.find('article', class_='jobs-description__container jobs-description__container--condensed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Aufgaben / Rolle / Benefits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKS\n",
    "#Define text replacements for section titles\n",
    "\n",
    "# Define replacements\n",
    "replacements = {\n",
    "    \"Profil\": \"Profil\",\n",
    "    \"PROFIL\": \"Profil\",\n",
    "    \"Qualifications\": \"Profil\",\n",
    "    \"WIR LIEBEN\": \"Profil\",\n",
    "    \"Das bringst du mit\": \"Profil\",\n",
    "    \"AUFGABEN\": \"Aufgaben\",\n",
    "    \"Aufgaben\": \"Aufgaben\",\n",
    "    \"Responsibilities\": \"Aufgaben\",\n",
    "    \"Das erwartet dich bei uns\": \"Aufgaben\",\n",
    "    \"Deine Aufgaben\": \"Aufgaben\",\n",
    "    \"Deine Aufgaben\": \"Aufgaben\",\n",
    "    \"DU LIEBST\": \"Aufgaben\",\n",
    "    \"BENEFITS\": \"Benefits\",\n",
    "    \"Perks\": \"Benefits\",\n",
    "    \"Das bieten wir dir\": \"Benefits\",\n",
    "    \"Benefits\": \"Benefits\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Entwicklung und Pflege von DashboardsEntwicklung und Erstellung von Reportings für KundenErstellung von Audiences und Insights als Basis für StrategieausarbeitungenAbleitung von HandlungsempfehlungenUnterstützung\\n bei der Erarbeitung von Kanalstrategien und strategischen Empfehlungen \\nfür Kunden im Bereich Datenanalyse/ReportingUnterstützung der Kolleg*innen bei der Interpretation von DatenAufbau und Vorantreiben der Dateninfrastruktur des Teams'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Aufgaben': ['Entwicklung und Pflege von Dashboards', 'Entwicklung und Erstellung von Reportings für Kunden', 'Erstellung von Audiences und Insights als Basis für Strategieausarbeitungen', 'Ableitung von Handlungsempfehlungen', 'Unterstützung bei der Erarbeitung von Kanalstrategien und strategischen Empfehlungen für Kunden im Bereich Datenanalyse/Reporting', 'Unterstützung der Kolleg*innen bei der Interpretation von Daten', 'Aufbau und Vorantreiben der Dateninfrastruktur des Teams'], 'Profil': ['Du hast bereits Berufserfahrung im Bereich Social Media in einer digitalen Agentur mit dem Schwerpunkt Datenanalyse gesammelt (Performance Agentur, Mediaagentur)', 'Du bringst Kenntnisse in der Social Media Analyse von Tools mit (zB Facebook Business Manager, Twitter Analytics, Instagram Analytics, Emplifi etc.), sowie Kenntnisse in der Visualisierung von Tools und Datenbanken (zB Tableau, Looker Studio, Power BI, IBM Kognos / SQL, MS SQL, P SQL)', 'Du hast bereits fundierte Erfahrung mit Web- und Marketing Analytics (zB Google Analytics, SAS, Salesforce, Adobe, Microsoft etc.)', 'Du besitzt fachliches Know How in Global Web Index oder Best4Planning', 'Besonders deine Erfahrungen mit Looker Studio und Supermetrics bringst du erfolgreich bei uns ein', 'Du besitzt die Fähigkeit Social Media KPIs und ihre Abhängigkeit zu verstehen (Paid und Organic) sowie das Verständnis, Erkenntnis in ansprechende und verständliche Darstellungen zu übertragen', 'Durch dein Verständnis für Prozesse der Social Media Datenerhebung und Darstellung bereicherst du das gesamte Team (Benchmarking, Index-Berechnung, Mittelwert, Lineare Regression, Box Plot und Skalenniveaus)'], 'Benefits': ['Wir ermöglichen Mobile Office und flexible Arbeitszeiten', 'Arbeiten aus dem EU-Ausland | bis zu 45 Tage', 'Discounts bei Dienstleistern und Fitnessstudios im Umkreis unserer Standorte', 'Möglichkeit der Mitgestaltung der Agenturkultur durch Round Tables und Mitarbeitendeninitiativen, wie z.\\u202fB. unserer &Queer*-Community', 'Starte bei uns mit einem erfahrenen Buddy an deiner Seite - wir nennen sie „First Friend“', 'Scholz & Friends Academy: offene Weiterbildungsangebote zu Fachthemen, Impulsgebungen, Diversity, Equity & Inclusion (D.E.I.) und vielem mehr', 'Standort-übergreifende Mentoring-Programme für Frauen auf unterschiedlichen Karrierestufen', 'Reverse-Mentoring-Programme zwischen Young Professionals und Führungskräften', 'Unconscious-Bias-Trainings für alle Mitarbeitenden', 'Mental-Health-Support in Kooperation mit unseren Partner*innen von Nilo Health: u.a. Videositzungen mit Therapeut*innen und Psycholog*innen, zur Förderung der individuellen mentalen Gesundheit', 'Eltern-Kind-Service zur Förderung des familienfreundlichen Klimas am Arbeitsplatz und eine gesunde Work-Life-Balance mit unseren Partner*innen von voiio']}\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Scholz & Friends\n",
    "html = \"/Users/juliankilchling/Downloads/Data Analyst (f_m_d) Scholz & Friends.html\"\n",
    "with open(html, \"r\", encoding=\"utf-8\") as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "job_description = soup.find('article', class_='jobs-description__container jobs-description__container--condensed')\n",
    "# Find all <ul> elements within the job description\n",
    "uls = job_description.find_all('ul')\n",
    "\n",
    "linkedin_dict={}\n",
    "\n",
    "# Define replacements\n",
    "replacements = {\n",
    "    \"Profil\": \"Profil\",\n",
    "    \"PROFIL\": \"Profil\",\n",
    "    \"Qualifications\": \"Profil\",\n",
    "    \"WIR LIEBEN\": \"Profil\",\n",
    "    \"Das bringst du mit\": \"Profil\",\n",
    "    \"AUFGABEN\": \"Aufgaben\",\n",
    "    \"Aufgaben\": \"Aufgaben\",\n",
    "    \"Responsibilities\": \"Aufgaben\",\n",
    "    \"Das erwartet dich bei uns\": \"Aufgaben\",\n",
    "    \"Deine Aufgaben\": \"Aufgaben\",\n",
    "    \"Deine Aufgaben\": \"Aufgaben\",\n",
    "    \"DU LIEBST\": \"Aufgaben\",\n",
    "    \"BENEFITS\": \"Benefits\",\n",
    "    \"Perks\": \"Benefits\",\n",
    "    \"Das bieten wir dir\": \"Benefits\",\n",
    "    \"Benefits\": \"Benefits\"\n",
    "}\n",
    "\n",
    "options = [\"Aufgaben\", \"Profil\", \"Benefits\", \"Jobbeschreibung\", \"Firmenprofil\"]\n",
    "\n",
    "\n",
    "\n",
    "def get_selected_option(loop_number):\n",
    "    # Create a Toplevel window\n",
    "    dialog = tk.Toplevel(root)\n",
    "    dialog.title(\"Wähle Section aus für:{}\".format(loop_number))\n",
    "\n",
    "    # Define a list of options\n",
    "    options = [\"Aufgaben\", \"Profil\", \"Benefits\", \"Jobbeschreibung\", \"Firmenprofil\"]\n",
    "\n",
    "    # Create a variable to store the selected option\n",
    "    selected_option = tk.StringVar(dialog)\n",
    "    selected_option.set(options[0])  # Set default option\n",
    "\n",
    "    # Create a dropdown menu (OptionMenu) for selecting an option\n",
    "    option_menu = tk.OptionMenu(dialog, selected_option, *options)\n",
    "    option_menu.pack()\n",
    "\n",
    "    def ok():\n",
    "        dialog.destroy()\n",
    "\n",
    "    # Create an \"OK\" button to confirm selection and close the dialog\n",
    "    ok_button = tk.Button(dialog, text=\"Auswählen\", command=ok)\n",
    "    ok_button.pack()\n",
    "\n",
    "    # Set the size of the dialog window\n",
    "    dialog.geometry(\"700x400\")  # Adjust the width and height as needed\n",
    "\n",
    "    # Wait for the dialog window to be closed\n",
    "    dialog.wait_window()\n",
    "\n",
    "    # Return the selected option\n",
    "    return selected_option.get()\n",
    "\n",
    "# Create a Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# Extract list items from each <ul> and put them into separate lists\n",
    "list_of_lists = []\n",
    "for index, ul in enumerate(uls, start=1):\n",
    "    list_entries = [li.get_text(strip=True) for li in ul.find_all('li')]\n",
    "    # Remove newline characters from list entries\n",
    "    list_entries = [entry.replace('\\n', '') for entry in list_entries]\n",
    "    # Get section heading to assign lists to the right section => use if statements for different html structures\n",
    "    if ul.parent.find_previous_sibling().text.strip() != '':\n",
    "        parent = ul.parent.find_previous_sibling().text.strip()\n",
    "        keyword = uls[0].parent.find_previous_sibling().text.strip()\n",
    "        # Perform multiple replacements\n",
    "        for key, value in replacements.items():\n",
    "            if key in parent:\n",
    "                # Replace the entire original string with the corresponding replacement value\n",
    "                parent = value\n",
    "                # Break the loop after the first replacement is done\n",
    "                break\n",
    "        if parent not in options:\n",
    "            parent = get_selected_option(ul.text.strip().replace('\\n', ' '))\n",
    "    elif ul.parent.find_previous_sibling().find_previous_sibling().text.strip() != '':\n",
    "        parent = ul.parent.find_previous_sibling().find_previous_sibling().text.strip()\n",
    "        keyword = uls[0].parent.find_previous_sibling().find_previous_sibling().text.strip()\n",
    "        # Perform multiple replacements\n",
    "        for key, value in replacements.items():\n",
    "            if key in parent:\n",
    "                # Replace the entire original string with the corresponding replacement value\n",
    "                parent = value\n",
    "                # Break the loop after the first replacement is done\n",
    "                break\n",
    "        if parent not in options:\n",
    "            parent = get_selected_option(ul.text.strip().replace('\\n', ' '))\n",
    "    elif ul.parent.find_previous_sibling().find_previous_sibling().find_previous_sibling().text.strip() != '':\n",
    "        parent = ul.parent.find_previous_sibling().find_previous_sibling().find_previous_sibling().text.strip()\n",
    "        keyword = uls[0].parent.find_previous_sibling().find_previous_sibling().find_previous_sibling().text.strip()\n",
    "        # Perform multiple replacements\n",
    "        for key, value in replacements.items():\n",
    "            if key in parent:\n",
    "                # Replace the entire original string with the corresponding replacement value\n",
    "                parent = value\n",
    "                # Break the loop after the first replacement is done\n",
    "                break\n",
    "        if parent not in options:\n",
    "            parent = get_selected_option(ul.text.strip().replace('\\n', ' '))\n",
    "    else:\n",
    "        parent = get_selected_option(ul.text.strip())\n",
    "    linkedin_dict[parent]= list_entries\n",
    "\n",
    "# Close the Tkinter root window\n",
    "root.destroy()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(linkedin_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent = \"hello\"\n",
    "for key, value in replacements.items():\n",
    "    if key in parent:\n",
    "        # Replace the entire original string with the corresponding replacement value\n",
    "        parent = value\n",
    "        # Break the loop after the first replacement is done\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Du  hast bereits Berufserfahrung im Bereich Social Media in einer digitalen  Agentur mit dem Schwerpunkt Datenanalyse gesammelt (Performance  Agentur, Mediaagentur)Du bringst  Kenntnisse in der Social Media Analyse von Tools mit (zB Facebook  Business Manager, Twitter Analytics, Instagram Analytics, Emplifi etc.),  sowie Kenntnisse in der Visualisierung von Tools und Datenbanken (zB  Tableau, Looker Studio, Power BI, IBM Kognos / SQL, MS SQL, P SQL)Du  hast bereits fundierte Erfahrung mit Web- und Marketing Analytics (zB  Google Analytics, SAS, Salesforce, Adobe, Microsoft etc.)Du besitzt fachliches Know How in Global Web Index oder Best4PlanningBesonders deine Erfahrungen mit Looker Studio und Supermetrics bringst du erfolgreich bei uns einDu  besitzt die Fähigkeit Social Media KPIs und ihre Abhängigkeit zu  verstehen (Paid und Organic) sowie das Verständnis, Erkenntnis in  ansprechende und verständliche Darstellungen zu übertragenDurch  dein Verständnis für Prozesse der Social Media Datenerhebung und  Darstellung bereicherst du das gesamte Team (Benchmarking,  Index-Berechnung, Mittelwert, Lineare Regression, Box Plot und  Skalenniveaus)'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uls[1].parent.find_previous_sibling().find_previous_sibling().find_previous_sibling().text.strip()\n",
    "uls[1].parent.find_previous_sibling().find_previous_sibling().text.strip()\n",
    "uls[1].text.strip().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected option: Benefits\n"
     ]
    }
   ],
   "source": [
    "%%python\n",
    "\n",
    "import tkinter as tk\n",
    "from tkinter import simpledialog\n",
    "\n",
    "def get_selected_option():\n",
    "    # Create a Toplevel window\n",
    "    dialog = tk.Toplevel(root)\n",
    "    dialog.title(\"Select Option - Loop {}\")\n",
    "\n",
    "    # Define a list of options\n",
    "    options = [\"Aufgaben\", \"Profil\", \"Benefits\", \"Jobbeschreibung\", \"Firmenprofil\"]\n",
    "\n",
    "    # Create a variable to store the selected option\n",
    "    selected_option = tk.StringVar(dialog)\n",
    "    selected_option.set(options[0])  # Set default option\n",
    "\n",
    "    # Create a dropdown menu (OptionMenu) for selecting an option\n",
    "    option_menu = tk.OptionMenu(dialog, selected_option, *options)\n",
    "    option_menu.pack()\n",
    "\n",
    "    def ok():\n",
    "        dialog.destroy()\n",
    "\n",
    "    # Create an \"OK\" button to confirm selection and close the dialog\n",
    "    ok_button = tk.Button(dialog, text=\"Auswählen\", command=ok)\n",
    "    ok_button.pack()\n",
    "\n",
    "    # Set the size of the dialog window\n",
    "    dialog.geometry(\"350x200\")  # Adjust the width and height as needed\n",
    "\n",
    "    # Wait for the dialog window to be closed\n",
    "    dialog.wait_window()\n",
    "\n",
    "    # Return the selected option\n",
    "    return selected_option.get()\n",
    "\n",
    "# Create a Tkinter root window\n",
    "root = tk.Tk()\n",
    "root.withdraw()  # Hide the root window\n",
    "\n",
    "# Prompt the user to select an option\n",
    "selected_option = get_selected_option()\n",
    "\n",
    "\n",
    "# Display the selected option\n",
    "if selected_option:\n",
    "    print(\"Selected option:\", selected_option)\n",
    "\n",
    "# Close the Tkinter root window\n",
    "root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ul><span><li><!-- -->Wir ermöglichen Mobile Office und flexible Arbeitszeiten<span class=\"white-space-pre\"> </span></li></span><span><li><!-- -->Arbeiten aus dem EU-Ausland | bis zu 45 Tage<span class=\"white-space-pre\"> </span></li></span><span><li><!-- -->Discounts bei Dienstleistern und Fitnessstudios im Umkreis unserer Standorte<span class=\"white-space-pre\"> </span></li></span><span><li><!-- -->Möglichkeit\n",
       " der Mitgestaltung der Agenturkultur durch Round Tables und \n",
       "Mitarbeitendeninitiativen, wie z. B. unserer &amp;Queer*-Community<span class=\"white-space-pre\"> </span></li></span><span><li><!-- -->Starte bei uns mit einem erfahrenen Buddy an deiner Seite - wir nennen sie „First Friend“<!-- --></li></span><span><li><!-- -->Scholz\n",
       " &amp; Friends Academy: offene Weiterbildungsangebote zu Fachthemen, \n",
       "Impulsgebungen, Diversity, Equity &amp; Inclusion (D.E.I.) und vielem \n",
       "mehr<span class=\"white-space-pre\"> </span></li></span><span><li><!-- -->Standort-übergreifende Mentoring-Programme für Frauen auf unterschiedlichen Karrierestufen<!-- --></li></span><span><li><!-- -->Reverse-Mentoring-Programme zwischen Young Professionals und Führungskräften<!-- --></li></span><span><li><!-- -->Unconscious-Bias-Trainings für alle Mitarbeitenden<span class=\"white-space-pre\"> </span></li></span><span><li><!-- -->Mental-\n",
       "Health-Support in Kooperation mit unseren Partner*innen von Nilo Health:\n",
       " u.a. Videositzungen mit Therapeut*innen und Psycholog*innen, zur \n",
       "Förderung der individuellen mentalen Gesundheit<span class=\"white-space-pre\"> </span></li></span><span><li><!-- -->Eltern-\n",
       "Kind-Service zur Förderung des familienfreundlichen Klimas am \n",
       "Arbeitsplatz und eine gesunde Work-Life-Balance mit unseren \n",
       "Partner*innen von voiio<span class=\"white-space-pre\"> </span><span><br/></span><span><br/></span></li></span></ul>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uls[1]\n",
    "uls[0].parent.find_previous_sibling().find_previous_sibling().find_previous_sibling().text.strip()\n",
    "uls[0].parent.find_previous_sibling().text.strip()\n",
    "uls[0].parent.find_previous_sibling().find_previous_sibling().text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Jobbeschreibung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONLY TESTING; SEE DIFFERENT APPROACH BELOW\n",
    "# ## WORKS ONLY FOR FUNKE => Get the job description\n",
    "\n",
    "# # Find the target <ul> element\n",
    "# target_ul = job_description.find('ul')\n",
    "\n",
    "# # Find the parent <span> element of the target <ul> element\n",
    "# parent_span = target_ul.find_parent('span')\n",
    "\n",
    "# # Initialize an empty string to store concatenated text\n",
    "# concatenated_description = ''\n",
    "\n",
    "# # Iterate over previous siblings of the parent <span> element\n",
    "# for sibling in parent_span.previous_siblings:\n",
    "#     # Check if the sibling is a <span> element\n",
    "#     if sibling.name == 'span':\n",
    "#         # Prepend the text of the <span> element to the string\n",
    "#         concatenated_description = sibling.get_text(strip=True) + ' ' + concatenated_description\n",
    "#     # Stop iteration if we encounter the target <span> element\n",
    "#     if sibling == soup.find('span'):\n",
    "#         break\n",
    "# linkedin_dict[\"job_description\"] = concatenated_description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset\n",
    "# linkedin_dict= {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ONLY TESTING\n",
    "\n",
    "# original_string = uls[0].parent.find_previous_sibling().text.strip()\n",
    "# # Iterate over key-value pairs in replacements\n",
    "# for key, value in replacements.items():\n",
    "#     # Check if the key is present in the original string\n",
    "#     if key in original_string:\n",
    "#         # Replace the entire original string with the corresponding replacement value\n",
    "#         new_string = value\n",
    "#         # Break the loop after the first replacement is done\n",
    "#         break\n",
    "# else:\n",
    "#     # If none of the keys are found in the original string, keep the original string unchanged\n",
    "#     new_string = original_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if uls[0].parent.find_previous_sibling().text.strip() != '':\n",
    "#     keyword = uls[0].parent.find_previous_sibling().text.strip()\n",
    "# elif uls[0].parent.find_previous_sibling().find_previous_sibling().text.strip() != '':\n",
    "#     keyword = uls[0].parent.find_previous_sibling().find_previous_sibling().text.strip()\n",
    "# else:\n",
    "#     keyword = uls[0].parent.find_previous_sibling().find_previous_sibling().find_previous_sibling().text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get job description => WORKS\n",
    "\n",
    "# Get the parent element\n",
    "parent_element = uls[0].parent.parent.text.replace('\\n', '').strip()\n",
    "\n",
    "# # Extract text from the parent element\n",
    "#parent_element.text.replace('\\n', '').strip()\n",
    "\n",
    "if keyword == '':\n",
    "    keyword = uls[0].text.strip()[:50]\n",
    "\n",
    "linkedin_dict[\"job_description\"] = parent_element.split(keyword)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Data Analyst - Mid or Senior',\n",
       " 'company': 'Adjust',\n",
       " 'pub_date': 'Vor 2 Wochen',\n",
       " 'jobtyp': '',\n",
       " 'Bestehende Kenntnisse': 'Data Science, Datenanalytik, Datenvisualisierung',\n",
       " 'Fehlende Kenntnisse': 'Analytik, Business Intelligence (BI), Dashboard, Data-Mining, Pandas (Software), Storytelling, Visualisierung',\n",
       " 'Aufgaben': ['Play a major role in the Data & Insights team by scoping analytics projects and shaping stakeholder decisions',\n",
       "  'Identify, analyze, and interpret complex data sets and present them in a clear, easy and understandable way',\n",
       "  'Able to handle difficult queries (large dataset, longer query times, optimization of approach)',\n",
       "  'Able to work with Pandas DataFrames to organize and analyze data, and using statistics to find important information',\n",
       "  'Familiarity with data visualization, skill in storytelling, and proficiency in developing user-friendly, self-service dashboards',\n",
       "  'Help our teams to ask the right questions, uncovering novel insights and opportunities for our business'],\n",
       " 'Profil': ['2-5 years of experience in Business Intelligence or Analytics',\n",
       "  'Strong ability to translate business questions into quantitative terms and provide actionable answers',\n",
       "  'Proficient in using business intelligence and visualization tools like Metabase, Tableau, and Domo',\n",
       "  'Advanced knowledge in data querying and transformation (SQL) and statistical computing (Python/Jupyter Notebooks)',\n",
       "  'Excellent verbal and written communication including ability to turn data insights into a compelling story',\n",
       "  'Efficient project management and ability to find creative solutions for diverse stakeholder groups'],\n",
       " 'Benefits': ['Opportunity to make an impact on the ad-tech industry working for a forward-thinking leader in the space',\n",
       "  'International, diverse teams with a strong focus on transparency, feedback and fun',\n",
       "  'Education opportunities for ongoing professional development & continuous learning',\n",
       "  'Company onboarding program where you’ll learn the ins-and-outs of our product with your fellow newbies',\n",
       "  'Flexible workplace policy, allowing you to work either from home or our office space',\n",
       "  'Generous vacation policy with flexible vacation days',\n",
       "  'Childcare program (Berlin only), paid parental leave',\n",
       "  'Additional private health insurance',\n",
       "  'Company Pension Plan with 40% employer contribution',\n",
       "  'English & German classes to enhance your collaboration with global colleagues and friends',\n",
       "  'Discounted Gym Membership with Urban Sports Club',\n",
       "  'Free annual BVG transportation ticket',\n",
       "  'Wellbeing support with qualified therapists',\n",
       "  'WFH program with remote online events (Coffee chats, team events, games, etc.)'],\n",
       " 'job_description': \"Adjust, an AppLovin (NASDAQ: APP) company, is trusted by marketers around the world to measure and grow their apps across platforms, from mobile to CTV and beyond. Adjust works with companies at every stage of the app marketing journey, from fast-growing digital brands to brick-and-mortar companies launching their first apps. Adjust’s powerful measurement and analytics suite provides visibility, insights and essential tools that drive better results.Our global team is composed of team members with life experiences, backgrounds, and perspectives that mirror our customers around the world. We seek outstanding candidates who demonstrate a commitment to diversity, equity, and inclusion through continuous learning and skill-building. We aim to support each other to ensure equitable and inclusive experiences, furthering our goal to create a culture of belonging that values the contributions of all team members.We're proud to have received several awards in Summer 2022, including G2's Easiest Setup, Easiest Admin, Leader, and Easiest To Do Business With. Our CPO was also honored with a Top Women in Media Award, and we were named the Best Overall Mobile Marketing Solution of 2022. These accolades are a testament to the excellence and innovation of our award-winning company.Get to know Product / The DepartmentOur Product Department is a dynamic team of agile, inquisitive, and strategic minds who believe strongly in finding problem driven solutions. We're the vital bridge between business and engineering, serving as the voice both internally and beyond for our Adjust product. We don't just think outside the box; we redefine it. Our mission is to relentlessly pursue features that perfectly align Adjust's vision with our clients' objectives.Make An Impact/ The RoleAs a Data Analyst, you will be part of the Data & Insights Team, composed of data analysts and engineers who help unlock valuable insights from our data by providing teams across Adjust with relevant insights and analyses to improve our products and processes. Your tasks will range from getting the data via ETL processes to analyzing and visualizing our data. Your goal will be to assist other departments in tackling the complexity of dealing with our data and supporting their data-driven decision making effectively.\"}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linkedin_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write to markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_job_title_linkedin():\n",
    "    # Get Job Title => WORKS\n",
    "    title = soup.find('h1', class_='t-24 t-bold job-details-jobs-unified-top-card__job-title').text.strip()\n",
    "    linkedin_dict[\"title\"] = title\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assign Filename from Company\n",
    "cleaned_title = re.sub(r'[^\\w\\s]', '', get_job_title_linkedin())\n",
    "output_filename = f\"jobsearch___{cleaned_title}.md\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get JobID of LinkedIn Job\n",
    "\n",
    "all_a_elements = soup.find_all(\"a\", {\"target\": \"_self\"})\n",
    "skill_match_elements = [element for element in all_a_elements if \"skill-match\" in element.get(\"href\", \"\")]\n",
    "\n",
    "skill_match_elements\n",
    "\n",
    "# Extract the 'href' attribute from the element\n",
    "href = skill_match_elements[0].get(\"href\", \"\")\n",
    "\n",
    "# Define the pattern for the job ID using regular expression\n",
    "pattern = r'jobId=(\\d+)'\n",
    "\n",
    "# Search for the pattern in the 'href' attribute\n",
    "match = re.search(pattern, href)\n",
    "\n",
    "# Extract the job ID as a string\n",
    "linkedin_dict[\"job_id\"] = match.group(1) if match else None\n",
    "linkedin_dict[\"url\"] = \"https://www.linkedin.com/jobs/view/\" + linkedin_dict[\"job_id\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make sure all dict keys are set\n",
    "\n",
    "# Check if the key exists in the dictionary\n",
    "if \"Benefits\" not in linkedin_dict:\n",
    "    linkedin_dict[\"Benefits\"] = [\"nicht gefunden\"]\n",
    "if \"Bestehende Kenntnisse\" not in linkedin_dict:\n",
    "    linkedin_dict[\"Bestehende Kenntnisse\"] = \"Keine\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tags section\n",
    "\n",
    "tags = f\"\"\"\\\n",
    "type:: stellenausschreibung\n",
    "institution:: {linkedin_dict[\"company\"]}\n",
    "Teilzeit:: {linkedin_dict[\"jobtyp\"]}\n",
    "status:: offen\n",
    "starting_date:: offen\n",
    "kommentar:: offen\n",
    "deadline:: offen\n",
    "ansprechpartner:: offen\n",
    "website:: [Stellenausschreibung]({linkedin_dict[\"url\"]})\n",
    "berufserfahrung:: offen\n",
    "kennziffer:: offen\n",
    "publication:: {linkedin_dict[\"pub_date\"]}\n",
    "matching_skills:: {linkedin_dict[\"Bestehende Kenntnisse\"]}\n",
    "missing_skills:: {linkedin_dict[\"Fehlende Kenntnisse\"]}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ToDos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create function to get future date starting from today and return it in the right format for Logseq's task management\n",
    "### if future date falls on a weekend, assign task to Monday instead => ONLY assign tasks on weekdays\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_date_logseq_format(days_from_today):\n",
    "    # Get today's date\n",
    "    today = datetime.now()\n",
    "    \n",
    "    # Calculate the date after delta_days\n",
    "    future_date = today + timedelta(days=days_from_today)\n",
    "\n",
    "    # Check if the future date falls on a weekend (Saturday or Sunday)\n",
    "    if future_date.weekday() >= 5:  # Saturday or Sunday\n",
    "        # Calculate the number of days to add to reach Monday\n",
    "        days_to_add = 7 - future_date.weekday()\n",
    "        future_date += timedelta(days=days_to_add)\n",
    "    \n",
    "    # Get the day name and format it\n",
    "    day_name = future_date.strftime('%a')\n",
    "    \n",
    "    # Format the date\n",
    "    formatted_date = future_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    # Combine the formatted date and day name\n",
    "    formatted_result = f'<{formatted_date} {day_name}>'\n",
    "    \n",
    "    return formatted_result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<2024-04-22 Mon>\n"
     ]
    }
   ],
   "source": [
    "# Example usage: Get the name of the day in two days from today\n",
    "result = get_date_logseq_format(4)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t- TODO ergänze relevante Informationen für Stellenausschreibung \"Data Analyst - Mid or Senior\" bei Adjust\n",
      "\t  SCHEDULED: <2024-04-18 Thu>\n",
      "\t\t- insb. zu Bewerbungsprozess und Unterlagen\n",
      "\t- TODO mache stichpunktartige Notizen zu Details in Stellenausschreibung \"Data Analyst - Mid or Senior\" bei Adjust\n",
      "\t  SCHEDULED: <2024-04-18 Thu>\n",
      "\t- TODO setze Anschreiben auf für \"Data Analyst - Mid or Senior\" bei Adjust\n",
      "\t  SCHEDULED: <2024-04-19 Fri>\n",
      "\t- TODO passe Lebenslauf an für \"Data Analyst - Mid or Senior\" bei Adjust\n",
      "\t  SCHEDULED: <2024-04-19 Fri>\n",
      "\t- TODO stelle Anschreiben fertig für \"Data Analyst - Mid or Senior\" bei Adjust\n",
      "\t  SCHEDULED: <2024-04-22 Mon>\n",
      "\t- TODO schicke Bewerbung ab für \"Data Analyst - Mid or Senior\" bei Adjust\n",
      "\t  SCHEDULED: <2024-04-22 Mon>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Write To Do Section in markdown file\n",
    "\n",
    "todos = f\"\"\"\\\n",
    "\t- TODO ergänze relevante Informationen für Stellenausschreibung \"{linkedin_dict[\"title\"]}\" bei {linkedin_dict[\"company\"]}\n",
    "\t  SCHEDULED: {get_date_logseq_format(0)}\n",
    "\t\t- insb. zu Bewerbungsprozess und Unterlagen\n",
    "\t- TODO mache stichpunktartige Notizen zu Details in Stellenausschreibung \"{linkedin_dict[\"title\"]}\" bei {linkedin_dict[\"company\"]}\n",
    "\t  SCHEDULED: {get_date_logseq_format(0)}\n",
    "\t- TODO setze Anschreiben auf für \"{linkedin_dict[\"title\"]}\" bei {linkedin_dict[\"company\"]}\n",
    "\t  SCHEDULED: {get_date_logseq_format(1)}\n",
    "\t- TODO passe Lebenslauf an für \"{linkedin_dict[\"title\"]}\" bei {linkedin_dict[\"company\"]}\n",
    "\t  SCHEDULED: {get_date_logseq_format(1)}\n",
    "\t- TODO stelle Anschreiben fertig für \"{linkedin_dict[\"title\"]}\" bei {linkedin_dict[\"company\"]}\n",
    "\t  SCHEDULED: {get_date_logseq_format(2)}\n",
    "\t- TODO schicke Bewerbung ab für \"{linkedin_dict[\"title\"]}\" bei {linkedin_dict[\"company\"]}\n",
    "\t  SCHEDULED: {get_date_logseq_format(2)}\n",
    "\"\"\"\n",
    "\n",
    "print(todos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_filename, 'w', encoding='utf-8') as markdown_file:\n",
    "    # Write tags to markdown\n",
    "    markdown_file.write(tags)\n",
    "    # Add to do section\n",
    "    markdown_file.write(f\"- ## To Dos\\n\")\n",
    "    markdown_file.write(\"  {{renderer :todomaster}}\")\n",
    "    markdown_file.write(f\"\\n\")\n",
    "    markdown_file.write(todos)\n",
    "    # Write Profil / Requirements to markdown\n",
    "    markdown_file.write(f\"- ## Jobbeschreibung\\n\")\n",
    "    markdown_file.write(f\"\\t- {linkedin_dict[\"job_description\"]}\\n\")\n",
    "    markdown_file.write(f\"- ## Profil\\n\")\n",
    "    for item in linkedin_dict[\"Profil\"]:\n",
    "        markdown_file.write(f\"\\t- {item}\\n\")\n",
    "    # Write Tasks / Aufgaben to markdown\n",
    "    markdown_file.write(f\"- ## Aufgaben\\n\")\n",
    "    for item in linkedin_dict[\"Aufgaben\"]:\n",
    "        markdown_file.write(f\"\\t- {item}\\n\")\n",
    "    # Write Benefits to markdown\n",
    "    markdown_file.write(f\"- ## Benefits\\n\")\n",
    "    for item in linkedin_dict[\"Benefits\"]:\n",
    "        markdown_file.write(f\"\\t- {item}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
